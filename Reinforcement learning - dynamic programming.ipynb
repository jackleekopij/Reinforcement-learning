{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning - dynamic programming\n",
    "\n",
    "At the core of reinforcement learning is the derivation of an optimal policy; a function which calculates actions to be executed from a state. The reinforcement learning problem is suited to solving a framework framework of problems termed Markov Decision Processes (MDPs). These processes are characterised by five key parameters, S (state), A (action), P (transition matrix), R (reward) and \\gamma (discount factor). If all five parameters are explicitly known, dynamic programming can be applied to MDPs to solve *(value) prediction* and *(optimal) control*. \n",
    "\n",
    "Dynamic programming provides a methodology to solve complex problems by breaking large/complex problems into smaller/more simple subproblems, solving subproblems and finally using subproblem solutions to construct solutions to the overall problem. Two important properties of dynamic programming: \n",
    "\n",
    "    1. Optimal substructure\n",
    "    2. Overlapping subproblems\n",
    "    \n",
    "    1. Optimal substructure\n",
    "    The idea of optimal substructure is premised on the idea complex problems can be decomposed into subproblems, calculate the optimal value for each subproblem then combine to form the overall optimal solution. \n",
    "    \n",
    "    2. Overlapping subproblems\n",
    "    Second, dynamic programming uses overlapping subproblems to cache solutions to previously calculated subproblems, store the output and reuse in calculations when the subproblems reoccur. \n",
    "    \n",
    "    \n",
    "Problems of the above form can be solved in both a prediction and control setting. Prediction solutions calculates the total value achievable from a state under a specific policy whereas control determines the action/decision that should be made in each state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy evaluation - prediction\n",
    "Value of functions are a at the core of reinforcement learning.\n",
    "Two value functions, state value functions - the utility for each particular state vs state-action value functions - the utility of taken a specific action from a state. Value functions have the structure of a Bellman equation, that is the value of the *current state*, $V(s)$, is equal to *immediate reward*, $R$, plus the value of *subsequent state* $V(s')$. Compactly the Bellman equation structure can be written as: \n",
    "\n",
    "\\begin{align}\n",
    "    V(s) &= R_t + V(s')\n",
    "\\end{align}\n",
    "\n",
    "Through analysis of the Bellman equation the first important property of dyanmic programming, *optimal substructure* is immediately apparent. A state's value function is broken into two less complex problems - the *immediate reward* and the *value function* of the trajectory from next state.\n",
    "\n",
    "Policy evaluation aims to solve the problem - \"if I follow a particular policy $\\pi$ calculate how 'good' is it to be in a particular state\". State alue functions are used to calculate each values states. \n",
    "\n",
    "Value functions used to solve MDPs are a subtle difference between the dynamic programming solutions and complete reinforcement paradigm. \n",
    "\n",
    "An MDP and policy are given the policy is then evaluated as to how optimal it is. \n",
    "\n",
    "For dynamic programming, since transitions are known and do NOT have to be sampled the MDP can be solved be sweeping/updating all states in each iteration. Full reinforcement learning is different as the state transitions are not known; dynamics must be learnt from experiencing the environment. Under such condition only observed states are updated in each iteration NOT the entire state space as is the case with the complete MDP solution. \n",
    "\n",
    "Policy evaluation is covered in the following code: \n",
    "    1. Define imports and parameters\n",
    "    2. Initialise state value function (matrix) - matrix of zeros\n",
    "    3. Update state value function (matrix) - iteration by iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Define imports and parameters\n",
    "from tkinter import *\n",
    "import numpy as np\n",
    "from dynamic_programming_helpers import *\n",
    "\n",
    "CANVAS_HEIGHT_WIDTH = 600\n",
    "GRID_DIM = 4\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "DISCOUNT_FACTOR = 1.0\n",
    "REWARD = -1\n",
    "\n",
    "line_distance = CANVAS_HEIGHT_WIDTH/GRID_DIM\n",
    "label_offset = line_distance/2\n",
    "\n",
    "# NOTE: due to padding of the matrix the x,y-coordinates have been incremented by one.\n",
    "goals_dim = [[1,1],[4,4]]\n",
    "\n",
    "#### Define policy\n",
    "trans_prob = {\"UP\":0.25, \"DOWN\":0.25, \"LEFT\":0.25, \"RIGHT\":0.25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise state value function\n",
    "grid_values = np.zeros([GRID_DIM,GRID_DIM])\n",
    "# Add edge of padding to simulate an edge node returning to state\n",
    "grid_values = np.pad(grid_values,((1,1),(1,1)), mode='constant', constant_values=0)\n",
    "\n",
    "\n",
    "grid_values = reset_goal_values(grid_values, goals_dim=goals_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run iterations\n",
    "With the goal states and state value functions initialised the policy evaluation algorithm can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#iteration: 1\n",
    "updated_grid_value = update_grid_state_value_function(grid_values, GRID_DIM, trans_prob, REWARD, DISCOUNT_FACTOR).round(2)\n",
    "\n",
    "updated_grid_value = reset_state_values(updated_grid_value).round(2)\n",
    "\n",
    "updated_grid_value = reset_goal_values(updated_grid_value, goals_dim=goals_dim).round(2)\n",
    "print(print_unpadded_matrix(updated_grid_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   -1.75 -2.   -2.  ]\n",
      " [-1.75 -2.   -2.   -2.  ]\n",
      " [-2.   -2.   -2.   -1.75]\n",
      " [-2.   -2.   -1.75  0.  ]]\n"
     ]
    }
   ],
   "source": [
    "#iteration: 2\n",
    "updated_grid_value = update_grid_state_value_function(updated_grid_value, GRID_DIM, trans_prob, REWARD, DISCOUNT_FACTOR).round(2)\n",
    "\n",
    "updated_grid_value = reset_state_values(updated_grid_value).round(2)\n",
    "\n",
    "updated_grid_value = reset_goal_values(updated_grid_value, goals_dim=goals_dim).round(2)\n",
    "print(print_unpadded_matrix(updated_grid_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_unpadded_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-aa9cd331da02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint_unpadded_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdated_grid_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'print_unpadded_matrix' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   -2.44 -2.94 -3.  ]\n",
      " [-2.44 -2.88 -3.   -2.94]\n",
      " [-2.94 -3.   -2.88 -2.44]\n",
      " [-3.   -2.94 -2.44  0.  ]]\n"
     ]
    }
   ],
   "source": [
    "#iteration: 3\n",
    "updated_grid_value = update_grid_state_value_function(updated_grid_value, GRID_DIM, trans_prob, REWARD, DISCOUNT_FACTOR).round(2)\n",
    "\n",
    "updated_grid_value = reset_state_values(updated_grid_value).round(2)\n",
    "\n",
    "updated_grid_value = reset_goal_values(updated_grid_value, goals_dim=goals_dim).round(2)\n",
    "print(print_unpadded_matrix(updated_grid_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "[[ 0.   -6.14 -8.36 -8.97]\n",
      " [-6.14 -7.74 -8.43 -8.36]\n",
      " [-8.35 -8.43 -7.74 -6.14]\n",
      " [-8.96 -8.35 -6.14  0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# iteration: 10\n",
    "for i in range(7):\n",
    "    print(i)\n",
    "    updated_grid_value = update_grid_state_value_function(updated_grid_value, GRID_DIM, trans_prob, REWARD, DISCOUNT_FACTOR).round(2)\n",
    "    updated_grid_value = reset_state_values(updated_grid_value).round(2)\n",
    "    updated_grid_value = reset_goal_values(updated_grid_value, goals_dim=goals_dim).round(2)\n",
    "\n",
    "print(print_unpadded_matrix(updated_grid_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = Tk()\n",
    "canvas_width = CANVAS_HEIGHT_WIDTH\n",
    "canvas_height = CANVAS_HEIGHT_WIDTH\n",
    "w = Canvas(master,\n",
    "           width=canvas_width,\n",
    "           height=canvas_height)\n",
    "w.pack()\n",
    "\n",
    "for j in range(GRID_DIM):\n",
    "    for i in range(GRID_DIM):\n",
    "        w.create_text(label_offset + i * line_distance, label_offset + j * line_distance,fill=\"darkblue\",font=\"Times 20 italic bold\",\n",
    "                        text=\"0.0\")\n",
    "        \n",
    "checkered(w, line_distance, canvas_height, canvas_width)\n",
    "mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
